# 📖 Traditional Korean Drinks Chatbot (Llama 2)

## 📌 프로젝트 개요  
이 프로젝트는 **Llama 2 7B 모델**을 활용하여 **한국 전통 음료**에 대한 정보를 제공하는 챗봇을 구현하는 코드이다.  
사용자가 입력한 질문을 바탕으로, 모델이 한국 전통 음료의 **재료, 제조 방법, 문화적 의미** 등을 설명하는 답변을 생성한다.
또한, 사용자의 질문과 모델의 응답을 DB에 저장하고, 모델의 응답 텍스트에서 등장 횟수의 상위 10개 단어를 통계내는 함수를 구현한다.

---

## 🤖 Llama 2 7B 모델

#### 소개
**Llama 2 7B** 모델은 **Meta**(구 Facebook)에서 개발한 **대형 언어 모델**인 **Llama 2** 시리즈의 일환이다. **7B**는 모델의 파라미터 수를 나타내며, 이 모델은 약 70억 개의 파라미터를 가지고 있다. Llama 2는 **GPT**(Generative Pretrained Transformer) 아키텍처를 기반으로 하며, 자연어 처리(NLP) 작업에서 매우 뛰어난 성능을 보여준다.

#### 특징
- **모델 크기**: Llama 2 7B는 70억 개의 파라미터를 가지고 있어서, 비교적 작은 모델이지만 여전히 많은 자연어 처리 작업에서 좋은 성능을 보여준다.
- **⭐️ 언어 생성**: Llama 2 7B는 **텍스트 생성, 번역, 요약, 질문 응답 등 다양한 자연어 처리 작업**을 할 수 있다.
- **Transformer 아키텍처**: Transformer 모델은 각 단어의 의미를 고려해 문맥을 분석하고, 각 단어에 대한 가중치를 부여해 더 나은 언어 이해와 생성을 가능하게 한다.
- **학습 데이터**: Llama 2는 대규모 텍스트 데이터셋을 기반으로 학습되어, 사람처럼 대화하는 방식으로 텍스트를 생성할 수 있다.

#### 활용
Llama 2 7B 모델은 다양한 **자연어 처리 작업**에 적합하며, 특히 텍스트 생성, 대화형 인공지능, 고객 서비스 챗봇 등에 유용하게 사용할 수 있다. **Hugging Face와 같은 플랫폼을 통해 모델을 쉽게 불러오고 사용**할 수 있어서, 다양한 개발자들이 빠르게 프로젝트에 적용할 수 있다.

#### 성능
**Llama 2 7B 모델은 GPT-3보다 적은 파라미터를 가지고 있지만, 효율적인 학습과 튜닝을 통해 경쟁력 있는 성능을 보여준다.** 이 모델은 특히 대화형 AI와 같은 생성적 태스크에서 높은 품질의 출력을 제공한다.

---


## 🤖 Llama-2-7b-chat-hf 모델 실행 시 하드웨어 환경에 따른 차이점

1. CPU와 GPU란?

**CPU(중앙처리장치, Central Processing Unit)**

CPU는 컴퓨터의 두뇌 역할을 하며, **일반적인 연산과 논리적 처리**를 담당함. 다중 작업을 수행할 수 있지만 **병렬 연산 성능이 제한적**이므로 **대량의 데이터 처리가 필요한 AI 모델을 실행하는 데에는 속도 저하**가 발생할 수 있음.

특징:

- 단일 또는 소수의 연산을 빠르게 처리

- 다목적 사용 (**운영체제 실행, 애플리케이션 처리** 등)

- 병렬 연산 성능이 낮음

**GPU(그래픽처리장치, Graphics Processing Unit)**

GPU는 **원래 그래픽 렌더링을 위해 개발된 하드웨어**이지만, **수천 개의 코어를 사용해 대량의 데이터를 병렬로 처리하는 데 최적화**되어 있음. 특히 **AI 모델 학습과 추론에서 CPU보다 훨씬 높은 성능을 발휘**함.

특징:

- 대규모 병렬 연산 가능

- AI 모델 학습 및 추론에 적합

- VRAM(비디오 메모리) 용량이 중요함

2. CPU만 사용할 경우

현재, 사용자의 환경(Mac M1)에 맞춰 CPU에서 프로젝트를 진행했으나, CPU에서 Llama-2-7b-chat-hf 모델을 실행하면 속도가 매우 느려짐. 모델 크기가 크기 때문에 추론(latency)이 높고, 텍스트 한 줄을 생성하는 데도 상당한 시간이 걸릴 수 있음.
➡️ 실제로 프로젝트를 사용자의 M1 환경에서 실행할 때, 25분 이상의 시간이 소요됐음.

특히, Mac M1과 같은 ARM 아키텍처 기반 CPU에서는 PyTorch의 기본 연산 속도가 제한적이므로, 효율적인 최적화 없이 실행할 경우 더욱 속도가 저하됨. 따라서, CPU 환경에서 실행할 경우 최적화된 라이브러리 (예: ONNX, GGUF 변환 후 llama.cpp 활용) 를 적용하는 것이 필수적임.

<details>
<summary>✅ Mac의 GPU와 NVIDIA GPU의 차이점</summary>

Mac에는 GPU가 있지만, 일반적인 NVIDIA GPU와 같은 범용 GPU와는 다르다. Mac의 경우, M1, M1 Pro, M1 Max, M2 시리즈 등에서 사용하는 Apple의 자체 GPU(MPS - Metal Performance Shaders)를 포함하고 있다. 이 GPU는 주로 그래픽 처리와 고속 연산을 지원하기 위해 설계되었다.

### 주요 차이점:
#### NVIDIA GPU와의 차이점:
- **NVIDIA GPU**는 **CUDA**라는 특화된 라이브러리를 사용하여 계산 작업을 최적화할 수 있다. 주로 딥러닝 모델 학습 및 추론에 많이 사용된다.
- 반면, **Apple GPU**는 딥러닝 작업을 위한 특화된 최적화가 부족하다. **Apple의 MPS(Metal Performance Shaders)** 라는 API를 통해 GPU 연산을 처리할 수 있지만, **PyTorch**와 **TensorFlow** 같은 라이브러리에서의 지원이 제한적이다.

#### Mac에서의 GPU 활용:
- **Apple Silicon(M1, M2)** 에는 자체 GPU가 탑재되어 있지만, **NVIDIA CUDA**와 같은 기존의 라이브러리로는 잘 활용되지 않는다. 대신 **MPS API**를 사용하여 GPU를 활용할 수 있다.
- 최신 버전의 **PyTorch**는 M1/M2 칩의 GPU를 지원하기 위해 **MPS**를 사용하지만, **CUDA**와 같은 강력한 최적화는 제공하지 않아서 성능 차이가 있을 수 있다.

### 요약:
- **Mac에 GPU가 있지만**, 기존 NVIDIA GPU에서 제공하는 **CUDA**와 같은 고급 기능은 사용할 수 없다.
- **M1, M2 칩의 Apple GPU**는 딥러닝에서 지원이 한정적이지만, **MPS**를 통해 일부 연산을 할 수 있다.
- **PyTorch**에서는 **MPS**를 사용하여 Mac에서 GPU 연산을 수행할 수 있지만, 성능 면에서 **CUDA** 기반 GPU보다는 떨어질 수 있다.

### 결론:
- **Mac M1/M2에서 GPU를 사용하려면** `device_map="mps"`나 `device_map="cpu"`를 선택할 수 있다.
- **GPU 성능이 중요하다면** NVIDIA GPU가 장착된 다른 컴퓨터를 사용하는 것이 더 효과적일 수 있다.
- ❗️해당 프로젝트에서는 Mac M1 환경의 안정성을 위해 CPU를 사용하도록 강제한다.
</details>


3. GPU만 사용할 경우

**GPU를 사용할 경우, 특히 NVIDIA의 CUDA 가속을 지원하는 환경에서 실행하면 속도가 비약적으로 향상됨.** 모델이 VRAM에 적재되므로, 충분한 VRAM(최소 24GB 이상)이 있다면 실시간 대화가 가능할 정도로 원활하게 동작함.

하지만 Mac M1/M2 환경에서는 CUDA 지원이 되지 않으므로, Metal 가속을 활용해야 함. Metal을 활용하면 일부 속도 향상이 있긴 하지만, CUDA 기반 GPU보다는 성능이 낮을 수밖에 없음.

4. CPU+GPU 혼용할 경우 (auto)

CPU와 GPU를 동시에 사용하는 방법도 있지만, 일반적으로 Llama-2-7b 모델을 제대로 실행하려면 GPU에 모든 가중치를 적재하는 것이 가장 이상적임. 다만, VRAM 용량이 부족할 경우 일부를 CPU RAM으로 오프로드(offloading)할 수 있음. 이 경우, 속도는 GPU 단독 사용보다 느려질 수 있지만, 메모리 부족으로 실행이 불가능한 상황을 방지할 수 있음.

특히, CPU 환경에서는 bitsandbytes 라이브러리를 이용한 4-bit 혹은 8-bit 양자화를 적용하면 GPU VRAM 사용량을 줄이면서도 성능을 유지할 수 있음. 이 방법은 RAM과 VRAM을 균형 있게 활용하는 데 유용함.

#### ➡️ 결론: 최적의 실행 환경은 GPU!

---

## 📝 전체 실행 흐름  

#### 1. 환경 설정 및 모델 로드  
- 필요한 라이브러리를 불러오고, `.env` 파일을 로드함.  
- Llama 2 모델과 토크나이저를 로드하여 사용할 준비를 함.  

#### 2. 데이터베이스 설정  
- `create_db()` 실행하여 `conversations` 테이블이 없으면 생성함.  
- `create_word_table()` 실행하여 `words` 테이블이 없으면 생성함.  

#### 3. 프롬프트 생성 및 입력 처리  
- 사용자 질문을 프롬프트 템플릿에 삽입하여 최종 입력 텍스트를 생성함.  
- 입력된 텍스트를 **토큰화(tokenization)** 하여 모델에 입력할 수 있는 형태로 변환함.  

#### 4. Llama 2 모델을 사용한 텍스트 생성  
- 모델이 입력을 기반으로 새로운 텍스트를 생성함.  
- 생성된 텍스트를 사람이 읽을 수 있도록 **디코딩(decoding)** 함.  

#### 5. 대화 내용 저장  
- 사용자 질문과 모델 응답을 `conversations` 테이블에 저장함.  

#### 6. 단어 추출 및 저장  
- 생성된 텍스트에서 단어를 추출하여 `words` 테이블에 저장함.  
- 이미 저장된 단어는 등장 횟수를 증가시킴.  

#### 7. 상위 단어 분석 및 시각화  
- 데이터베이스에서 가장 많이 등장한 단어 10개를 가져옴.  
- 단어 등장 횟수를 **막대 그래프(bar chart)** 로 시각화하여 출력함.  

#### 8. 실행 시간 측정 및 출력  
- 코드 실행 시간을 측정하여 출력함.  

코드를 실행하면 다음과 같은 통계표가 함께 출력된다.
![통계표](./data/단어_등장횟수_통계표.PNG)

---

## 🏆 기대 결과

이 코드를 실행하면, 다음과 같은 한국 전통 음료 추천 메시지를 받을 수 있다.

```plaintext
One of the traditional Korean drinks I highly recommend is Sikhye. 
Sikhye is a sweet rice punch made from malt water, cooked rice, and sugar. 
It has a slightly sweet and refreshing taste and is often served cold. 
Sikhye has been enjoyed in Korea for centuries and is commonly served as a dessert drink during holidays like Chuseok and Lunar New Year. 
Would you like more recommendations?
```

(결과는 실행할 때마다 다를 수 있음.)

---

## 📌 한계점 및 개선 사항

1. 모델의 실행 속도

- 현재 Llama 2 7B 모델은 Mac M1에서 실행하기에는 매우 무겁다.
- 실행 속도를 개선하려면 더 작은 모델 (e.g., Llama 2 7B 대신 3B) 을 사용하거나 **GPU 지원 환경**에서 실행하는 것이 좋다.

2. 출력의 일관성 부족

- 가끔씩 문맥에 맞지 않는 답변이 생성될 수 있음.
- 보다 정확한 응답을 원한다면 시스템 메시지를 더 구체적으로 조정해야 함.

3. 프롬프트 엔지니어링

- 현재는 단순한 질문 응답 형태이지만, 대화형 챗봇으로 발전시키려면 추가적인 컨텍스트 저장 기능이 필요함.

---

## 📢 마무리

이 프로젝트는 Llama 2 7B 모델을 활용하여 한국 전통 음료 정보를 제공하는 챗봇을 구축하는 코드이다.
현재 프로젝트 사용자의 Mac M1 환경에서 실행하도록 설정되었으며, 실행 속도와 모델 크기를 고려하여 최적화를 진행해야 한다.
